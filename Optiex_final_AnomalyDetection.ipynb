{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPG7rDpzaqveE83m4CX8sYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachiiiket/OptiexIOT_FinalScripts/blob/main/Optiex_final_AnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements to run the script"
      ],
      "metadata": {
        "id": "GvbLPGAB6em0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EomxsLQ5_Lc"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y wget gnupg\n",
        "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | apt-key add -\n",
        "!echo \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n",
        "!apt-get update\n",
        "!apt-get install -y mongodb-database-tools\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connection requirements for mongo client"
      ],
      "metadata": {
        "id": "bNL1FAYE6mfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient, UpdateOne\n",
        "from collections import deque\n",
        "\n",
        "client = MongoClient(\"Specify_your_connection_URL\")\n",
        "\n",
        "# Choose your database and collection\n",
        "db = client[\"testdb\"]          # replace with your DB name\n",
        "collection = db[\"sensors\"]     # replace with your collection name"
      ],
      "metadata": {
        "id": "Hp3k67W26Rm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Script - keep the Chunk fetch and bulk limit as it is for larger data"
      ],
      "metadata": {
        "id": "vlxVcSwm6uAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Config ---\n",
        "CHUNK_FETCH = 5000    # fetch this many docs at once\n",
        "BULK_LIMIT  = 1000     # flush bulk writes every 1000 updates\n",
        "\n",
        "bulk_updates = []\n",
        "carryover = []   # keep last 2 docs from previous batch\n",
        "count = 0\n",
        "\n",
        "def flush_updates():\n",
        "    \"\"\"Write accumulated bulk updates to DB.\"\"\"\n",
        "    global bulk_updates\n",
        "    if bulk_updates:\n",
        "        collection.bulk_write(bulk_updates, ordered=False)\n",
        "        bulk_updates = []\n",
        "\n",
        "# --- Stream through DB in batches ---\n",
        "batch_cursor = collection.find({}, {\"Server Timestamp\": 1, \"Value\": 1}).sort(\"Server Timestamp\", -1)  # DESCENDING like best script\n",
        "batch = []\n",
        "\n",
        "for doc in batch_cursor:\n",
        "    batch.append(doc)\n",
        "\n",
        "    if len(batch) == CHUNK_FETCH:\n",
        "        # merge with carryover from previous batch\n",
        "        chunk = carryover + batch\n",
        "\n",
        "        # keep last 2 docs for overlap\n",
        "        carryover = chunk[-2:]\n",
        "        process_chunk = chunk[:-2]\n",
        "\n",
        "        i = 1\n",
        "        n = len(process_chunk)\n",
        "\n",
        "        while i < n - 1:\n",
        "            prev = process_chunk[i - 1][\"Value\"]\n",
        "            curr = process_chunk[i][\"Value\"]\n",
        "            nxt  = process_chunk[i + 1][\"Value\"]\n",
        "\n",
        "            anomaly_flag = None\n",
        "            new_value = None\n",
        "\n",
        "            # ----------- RESET -----------\n",
        "            if curr < prev and nxt > prev:\n",
        "                anomaly_flag = \"Yes (Reset)\"\n",
        "\n",
        "            # ----------- FAULTY SENSOR -----------\n",
        "            elif curr == 0:\n",
        "                start = i\n",
        "                while i < n and process_chunk[i][\"Value\"] == 0:\n",
        "                    i += 1\n",
        "                end = i - 1\n",
        "\n",
        "                if i < n:  # we have an \"after\" value\n",
        "                    after = process_chunk[i][\"Value\"]\n",
        "                    if after < prev:\n",
        "                        new_value = (prev + after) / 2\n",
        "                        for j in range(start, end + 1):\n",
        "                            bulk_updates.append(UpdateOne(\n",
        "                                {\"_id\": process_chunk[j][\"_id\"]},\n",
        "                                {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                            ))\n",
        "                    else:\n",
        "                        bulk_updates.append(UpdateOne(\n",
        "                            {\"_id\": process_chunk[start][\"_id\"]},\n",
        "                            {\"$set\": {\"Anomaly Flag\": \"Yes (Reset)\"}}\n",
        "                        ))\n",
        "                else:  # reached end of dataset\n",
        "                    new_value = prev\n",
        "                    for j in range(start, end + 1):\n",
        "                        bulk_updates.append(UpdateOne(\n",
        "                            {\"_id\": process_chunk[j][\"_id\"]},\n",
        "                            {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                        ))\n",
        "                continue  # skip normal increment, since i already moved\n",
        "\n",
        "            # ----------- NEGATIVE DIFFERENCE -----------\n",
        "            elif nxt > curr:\n",
        "                anomaly_flag = \"Yes (Negative Difference)\"\n",
        "\n",
        "            # --- Add to bulk ---\n",
        "            update_fields = {}\n",
        "            if anomaly_flag:\n",
        "                update_fields[\"Anomaly Flag\"] = anomaly_flag\n",
        "            if new_value is not None:\n",
        "                update_fields[\"Value\"] = new_value   # overwrite Value as requested\n",
        "\n",
        "            if update_fields:\n",
        "                bulk_updates.append(UpdateOne({\"_id\": process_chunk[i][\"_id\"]}, {\"$set\": update_fields}))\n",
        "\n",
        "            # --- Flush bulk if needed ---\n",
        "            if len(bulk_updates) >= BULK_LIMIT:\n",
        "                flush_updates()\n",
        "\n",
        "            i += 1\n",
        "            count += 1\n",
        "\n",
        "            # --- Log every 50k processed rows ---\n",
        "            if count % 50000 == 0:\n",
        "                print(f\"Processed {count} rows...\")\n",
        "\n",
        "        # reset batch\n",
        "        batch = []\n",
        "\n",
        "# process leftovers\n",
        "if carryover:\n",
        "    process_chunk = carryover\n",
        "    # (same loop as above could be run if needed, but usually overlap is small)\n",
        "\n",
        "flush_updates()\n",
        "print(f\"Finished processing {count} rows.\")\n"
      ],
      "metadata": {
        "id": "ODF2b_kK6Uig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}