{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9DHAVqNmz9QDo1evpvk+R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachiiiket/OptiexIOT_FinalScripts/blob/main/Optiex_final_AnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements to run the script"
      ],
      "metadata": {
        "id": "GvbLPGAB6em0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EomxsLQ5_Lc"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y wget gnupg\n",
        "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | apt-key add -\n",
        "!echo \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n",
        "!apt-get update\n",
        "!apt-get install -y mongodb-database-tools\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connection requirements for mongo client - Specify all mongo connection variables to connect with the mongo db database"
      ],
      "metadata": {
        "id": "bNL1FAYE6mfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient, UpdateOne\n",
        "from collections import deque\n",
        "\n",
        "client = MongoClient(\"Specify_your_connection_URL\")\n",
        "\n",
        "# Choose your database and collection\n",
        "db = client[\"testdb\"]          # replace with your DB name\n",
        "collection = db[\"sensors\"]     # replace with your collection name"
      ],
      "metadata": {
        "id": "Hp3k67W26Rm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Script - keep the Chunk fetch and bulk limit as it is for larger data"
      ],
      "metadata": {
        "id": "vlxVcSwm6uAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Config ---\n",
        "CHUNK_FETCH = 5000    # fetch this many docs at once\n",
        "BULK_LIMIT  = 1000     # flush bulk writes every 1000 updates\n",
        "\n",
        "bulk_updates = []\n",
        "carryover = []   # keep last 2 docs from previous batch\n",
        "count = 0\n",
        "\n",
        "def flush_updates():\n",
        "    \"\"\"Write accumulated bulk updates to DB.\"\"\"\n",
        "    global bulk_updates\n",
        "    if bulk_updates:\n",
        "        collection.bulk_write(bulk_updates, ordered=False)\n",
        "        bulk_updates = []\n",
        "\n",
        "# --- Stream through DB in batches ---\n",
        "batch_cursor = collection.find({}, {\"Server Timestamp\": 1, \"Value\": 1}).sort(\"Server Timestamp\", -1)  # DESCENDING like best script\n",
        "batch = []\n",
        "\n",
        "for doc in batch_cursor:\n",
        "    batch.append(doc)\n",
        "\n",
        "    if len(batch) == CHUNK_FETCH:\n",
        "        # merge with carryover from previous batch\n",
        "        chunk = carryover + batch\n",
        "\n",
        "        # keep last 2 docs for overlap\n",
        "        carryover = chunk[-2:]\n",
        "        process_chunk = chunk[:-2]\n",
        "\n",
        "        i = 1\n",
        "        n = len(process_chunk)\n",
        "\n",
        "        while i < n - 1:\n",
        "            prev = process_chunk[i - 1][\"Value\"]\n",
        "            curr = process_chunk[i][\"Value\"]\n",
        "            nxt  = process_chunk[i + 1][\"Value\"]\n",
        "\n",
        "            anomaly_flag = None\n",
        "            new_value = None\n",
        "\n",
        "            # ----------- RESET -----------\n",
        "            if curr < prev and nxt > prev:\n",
        "                anomaly_flag = \"Yes (Reset)\"\n",
        "\n",
        "            # ----------- FAULTY SENSOR -----------\n",
        "            elif curr == 0:\n",
        "                start = i\n",
        "                while i < n and process_chunk[i][\"Value\"] == 0:\n",
        "                    i += 1\n",
        "                end = i - 1\n",
        "\n",
        "                if i < n:  # we have an \"after\" value\n",
        "                    after = process_chunk[i][\"Value\"]\n",
        "                    if after < prev:\n",
        "                        new_value = (prev + after) / 2\n",
        "                        for j in range(start, end + 1):\n",
        "                            bulk_updates.append(UpdateOne(\n",
        "                                {\"_id\": process_chunk[j][\"_id\"]},\n",
        "                                {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                            ))\n",
        "                    else:\n",
        "                        bulk_updates.append(UpdateOne(\n",
        "                            {\"_id\": process_chunk[start][\"_id\"]},\n",
        "                            {\"$set\": {\"Anomaly Flag\": \"Yes (Reset)\"}}\n",
        "                        ))\n",
        "                else:  # reached end of dataset\n",
        "                    new_value = prev\n",
        "                    for j in range(start, end + 1):\n",
        "                        bulk_updates.append(UpdateOne(\n",
        "                            {\"_id\": process_chunk[j][\"_id\"]},\n",
        "                            {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                        ))\n",
        "                continue  # skip normal increment, since i already moved\n",
        "\n",
        "            # ----------- NEGATIVE DIFFERENCE -----------\n",
        "            elif nxt > curr:\n",
        "                anomaly_flag = \"Yes (Negative Difference)\"\n",
        "\n",
        "            # --- Add to bulk ---\n",
        "            update_fields = {}\n",
        "            if anomaly_flag:\n",
        "                update_fields[\"Anomaly Flag\"] = anomaly_flag\n",
        "            if new_value is not None:\n",
        "                update_fields[\"Value\"] = new_value   # overwrite Value as requested\n",
        "\n",
        "            if update_fields:\n",
        "                bulk_updates.append(UpdateOne({\"_id\": process_chunk[i][\"_id\"]}, {\"$set\": update_fields}))\n",
        "\n",
        "            # --- Flush bulk if needed ---\n",
        "            if len(bulk_updates) >= BULK_LIMIT:\n",
        "                flush_updates()\n",
        "\n",
        "            i += 1\n",
        "            count += 1\n",
        "\n",
        "            # --- Log every 50k processed rows ---\n",
        "            if count % 50000 == 0:\n",
        "                print(f\"Processed {count} rows...\")\n",
        "\n",
        "        # reset batch\n",
        "        batch = []\n",
        "\n",
        "# process leftovers\n",
        "if carryover:\n",
        "    process_chunk = carryover\n",
        "    # (same loop as above could be run if needed, but usually overlap is small)\n",
        "\n",
        "flush_updates()\n",
        "print(f\"Finished processing {count} rows.\")\n"
      ],
      "metadata": {
        "id": "ODF2b_kK6Uig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fixed - made changes to best script - Works good on 3774\n",
        "from pymongo import MongoClient, UpdateOne\n",
        "\n",
        "# ---- MongoDB Atlas Connection ----\n",
        "client = MongoClient(\"Mongo-URL\")\n",
        "\n",
        "db = client[\"testdb\"]          # <-- replace with your DB name\n",
        "collection = db[\"sensors\"]     # <-- replace with your collection name\n",
        "\n",
        "# --- Config ---\n",
        "CHUNK_FETCH = 500    # fetch this many docs at once\n",
        "BULK_LIMIT  = 1000   # flush bulk writes every 1000 updates\n",
        "\n",
        "bulk_updates = []\n",
        "carryover = []   # keep last 2 docs for overlap\n",
        "count = 0\n",
        "\n",
        "def flush_updates():\n",
        "    global bulk_updates\n",
        "    if bulk_updates:\n",
        "        collection.bulk_write(bulk_updates, ordered=False)\n",
        "        bulk_updates = []\n",
        "\n",
        "def process_docs(docs, carryover):\n",
        "    global bulk_updates, count\n",
        "    if not docs and not carryover:\n",
        "        return []\n",
        "\n",
        "    # merge with carryover\n",
        "    chunk = carryover + docs\n",
        "    # python-side sort (descending by timestamp like original script)\n",
        "    chunk = sorted(chunk, key=lambda x: x[\"Server Timestamp\"], reverse=True)\n",
        "\n",
        "    # keep last 2 docs for overlap\n",
        "    carryover = chunk[-2:] if len(chunk) >= 2 else chunk[:]\n",
        "    process_chunk = chunk[:-2] if len(chunk) >= 2 else []\n",
        "\n",
        "    i = 1\n",
        "    n = len(process_chunk)\n",
        "\n",
        "    while i < n - 1:\n",
        "        prev = process_chunk[i - 1][\"Value\"]\n",
        "        curr = process_chunk[i][\"Value\"]\n",
        "        nxt  = process_chunk[i + 1][\"Value\"]\n",
        "\n",
        "        anomaly_flag = None\n",
        "        new_value = None\n",
        "\n",
        "        # ----------- RESET -----------\n",
        "        if curr < prev and nxt > prev:\n",
        "            anomaly_flag = \"Yes (Reset)\"\n",
        "\n",
        "        # ----------- FAULTY SENSOR -----------\n",
        "        elif curr == 0:\n",
        "            start = i\n",
        "            while i < n and process_chunk[i][\"Value\"] == 0:\n",
        "                i += 1\n",
        "            end = i - 1\n",
        "\n",
        "            if i < n:  # we have an \"after\" value\n",
        "                after = process_chunk[i][\"Value\"]\n",
        "                if after < prev:\n",
        "                    new_value = (prev + after) / 2\n",
        "                    for j in range(start, end + 1):\n",
        "                        bulk_updates.append(UpdateOne(\n",
        "                            {\"_id\": process_chunk[j][\"_id\"]},\n",
        "                            {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                        ))\n",
        "                else:\n",
        "                    bulk_updates.append(UpdateOne(\n",
        "                        {\"_id\": process_chunk[start][\"_id\"]},\n",
        "                        {\"$set\": {\"Anomaly Flag\": \"Yes (Reset)\"}}\n",
        "                    ))\n",
        "            else:  # reached end of dataset\n",
        "                new_value = prev\n",
        "                for j in range(start, end + 1):\n",
        "                    bulk_updates.append(UpdateOne(\n",
        "                        {\"_id\": process_chunk[j][\"_id\"]},\n",
        "                        {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                    ))\n",
        "            continue  # skip normal increment, since i already moved\n",
        "\n",
        "        # ----------- NEGATIVE DIFFERENCE -----------\n",
        "        elif nxt > curr:\n",
        "            anomaly_flag = \"Yes (Negative Difference)\"\n",
        "\n",
        "        # --- Add to bulk ---\n",
        "        update_fields = {}\n",
        "        if anomaly_flag:\n",
        "            update_fields[\"Anomaly Flag\"] = anomaly_flag\n",
        "        if new_value is not None:\n",
        "            update_fields[\"Value\"] = new_value   # overwrite Value\n",
        "\n",
        "        if update_fields:\n",
        "            bulk_updates.append(UpdateOne({\"_id\": process_chunk[i][\"_id\"]}, {\"$set\": update_fields}))\n",
        "\n",
        "        # --- Flush bulk if needed ---\n",
        "        if len(bulk_updates) >= BULK_LIMIT:\n",
        "            flush_updates()\n",
        "\n",
        "        i += 1\n",
        "        count += 1\n",
        "\n",
        "        if count % 50000 == 0:\n",
        "            print(f\"Processed {count} rows...\")\n",
        "\n",
        "    return carryover\n",
        "\n",
        "\n",
        "# --- Pagination loop ---\n",
        "last_id = None\n",
        "while True:\n",
        "    query = {\"_id\": {\"$lt\": last_id}} if last_id else {}\n",
        "    docs = list(collection.find(query, {\"Server Timestamp\": 1, \"Value\": 1})\n",
        "                          .sort(\"_id\", -1)   # paginate backwards for descending\n",
        "                          .limit(CHUNK_FETCH))\n",
        "\n",
        "    if not docs:\n",
        "        break\n",
        "\n",
        "    carryover = process_docs(docs, carryover)\n",
        "    last_id = docs[-1][\"_id\"]   # move to next page\n",
        "\n",
        "# process any leftover docs\n",
        "if carryover:\n",
        "    process_docs([], carryover)\n",
        "\n",
        "flush_updates()\n",
        "print(f\"Finished processing {count} rows.\")\n"
      ],
      "metadata": {
        "id": "6mCaj6iVPg9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pymongo import MongoClient, UpdateOne\n",
        "from datetime import timedelta\n",
        "\n",
        "# ---- MongoDB Atlas Connection ----\n",
        "client = MongoClient(\"Mongo-URL\")\n",
        "\n",
        "db = client[\"testdb\"]          # <-- replace with your DB name\n",
        "collection = db[\"sensors\"]     # <-- replace with your collection name\n",
        "\n",
        "# --- Step 1: Fetch all reset anomalies ---\n",
        "reset_docs = list(collection.find(\n",
        "    {\"Anomaly Flag\": \"Yes (Reset)\"},\n",
        "    {\"_id\": 1, \"Server Timestamp\": 1, \"Value\": 1}\n",
        "))\n",
        "\n",
        "if not reset_docs:\n",
        "    print(\"No reset anomalies found.\")\n",
        "    exit()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(reset_docs)\n",
        "df[\"Server Timestamp\"] = pd.to_datetime(df[\"Server Timestamp\"])\n",
        "df = df.sort_values(\"Server Timestamp\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_file = \"reset_anomalies.csv\"\n",
        "df.to_csv(csv_file, index=False)\n",
        "print(f\"Saved reset anomalies to {csv_file}\")\n",
        "\n",
        "# --- Step 2: Detect multiple resets within 7 days ---\n",
        "updates = []\n",
        "n = len(df)\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(i + 1, n):\n",
        "        t1 = df.iloc[i][\"Server Timestamp\"]\n",
        "        t2 = df.iloc[j][\"Server Timestamp\"]\n",
        "\n",
        "        if abs((t2 - t1).days) <= 7:\n",
        "            # Both are faulty resets\n",
        "            for idx in [i, j]:\n",
        "                updates.append(UpdateOne(\n",
        "                    {\"_id\": df.iloc[idx][\"_id\"]},\n",
        "                    {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty Reset)\"}}\n",
        "                ))\n",
        "\n",
        "# --- Step 3: Apply updates in bulk ---\n",
        "if updates:\n",
        "    collection.bulk_write(updates, ordered=False)\n",
        "    print(f\"Updated {len(updates)} reset anomalies to Faulty Reset.\")\n",
        "else:\n",
        "    print(\"No multiple resets within 7 days found.\")\n"
      ],
      "metadata": {
        "id": "7tqt_2ORdav2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}