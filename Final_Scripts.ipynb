{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWaA1Va2XHc+466kv2tkBe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y wget gnupg\n",
        "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | apt-key add -\n",
        "!echo \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n",
        "!apt-get update\n",
        "!apt-get install -y mongodb-database-tools\n",
        "!pip install pymongo"
      ],
      "metadata": {
        "id": "7WheAj_m6VKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient, UpdateOne\n",
        "from collections import deque\n",
        "\n",
        "client = MongoClient(\"\")\n",
        "\n",
        "# Choose your database and collection\n",
        "db = client[\"testdb\"]          # <-- replace with your DB name\n",
        "collection = db[\"sensors\"]     # <-- replace with your collection name"
      ],
      "metadata": {
        "id": "wZvsJQMg6FEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqDRpuuL56un"
      },
      "outputs": [],
      "source": [
        "# # --- Config ---\n",
        "# CHUNK_FETCH = 5000   # fetch this many docs at once\n",
        "# PROCESS_SIZE = 4950  # process only 4900 docs (leave overlap)\n",
        "# BULK_LIMIT = 1000    # flush bulk writes every 1000 updates\n",
        "\n",
        "# bulk_updates = []\n",
        "# carryover = []   # keep last 2 docs from previous batch\n",
        "# count = 0\n",
        "\n",
        "# def flush_updates():\n",
        "#     \"\"\"Write accumulated bulk updates to DB.\"\"\"\n",
        "#     global bulk_updates\n",
        "#     if bulk_updates:\n",
        "#         collection.bulk_write(bulk_updates, ordered=False)\n",
        "#         bulk_updates = []\n",
        "\n",
        "# # --- Stream through DB in batches ---\n",
        "# batch_cursor = collection.find({}, {\"Server Timestamp\": 1, \"Value\": 1})\n",
        "# batch = []\n",
        "\n",
        "# for doc in batch_cursor:\n",
        "#     batch.append(doc)\n",
        "#     if len(batch) == CHUNK_FETCH:\n",
        "#         # merge with carryover from previous batch\n",
        "#         chunk = carryover + batch\n",
        "\n",
        "#         # sort in memory by timestamp\n",
        "#         chunk = sorted(chunk, key=lambda x: x[\"Server Timestamp\"])\n",
        "\n",
        "#         # keep last 2 docs for overlap to next chunk\n",
        "#         carryover = chunk[-2:]\n",
        "#         process_chunk = chunk[:-2]   # leave last 2 aside\n",
        "\n",
        "#         # process this chunk\n",
        "#         window = []\n",
        "#         for curr_doc in process_chunk:\n",
        "#             window.append(curr_doc)\n",
        "#             if len(window) < 3:\n",
        "#                 continue\n",
        "\n",
        "#             prev, curr, nxt = window[0], window[1], window[2]\n",
        "#             anomaly_flag = None\n",
        "#             new_value = None\n",
        "\n",
        "#             # ----------- RESET -----------\n",
        "#             if curr[\"Value\"] < prev[\"Value\"] and nxt[\"Value\"] > prev[\"Value\"]:\n",
        "#                 anomaly_flag = \"Yes (Reset)\"\n",
        "\n",
        "#             # ----------- FAULTY SENSOR -----------\n",
        "#             elif curr[\"Value\"] == 0:\n",
        "#                 zero_block = [curr]\n",
        "#                 while len(window) >= 3 and window[1][\"Value\"] == 0:\n",
        "#                     zero_block.append(window[1])\n",
        "#                     window.pop(1)\n",
        "\n",
        "#                 if len(window) >= 3:\n",
        "#                     after_val = window[1][\"Value\"]\n",
        "#                     if after_val < prev[\"Value\"]:\n",
        "#                         new_value = (prev[\"Value\"] + after_val) / 2\n",
        "#                         for z in zero_block:\n",
        "#                             bulk_updates.append(UpdateOne(\n",
        "#                                 {\"_id\": z[\"_id\"]},\n",
        "#                                 {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "#                             ))\n",
        "#                     else:\n",
        "#                         bulk_updates.append(UpdateOne(\n",
        "#                             {\"_id\": zero_block[0][\"_id\"]},\n",
        "#                             {\"$set\": {\"Anomaly Flag\": \"Yes (Reset)\"}}\n",
        "#                         ))\n",
        "#                 else:  # dataset finished → copy prev\n",
        "#                     new_value = prev[\"Value\"]\n",
        "#                     for z in zero_block:\n",
        "#                         bulk_updates.append(UpdateOne(\n",
        "#                             {\"_id\": z[\"_id\"]},\n",
        "#                             {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "#                         ))\n",
        "#                 continue\n",
        "\n",
        "#             # ----------- NEGATIVE DIFFERENCE -----------\n",
        "#             elif nxt[\"Value\"] > curr[\"Value\"]:\n",
        "#                 anomaly_flag = \"Yes (Negative Difference)\"\n",
        "\n",
        "#             # --- Add to bulk ---\n",
        "#             update_fields = {}\n",
        "#             if anomaly_flag:\n",
        "#                 update_fields[\"Anomaly Flag\"] = anomaly_flag\n",
        "#             if new_value is not None:\n",
        "#                 update_fields[\"Value\"] = new_value\n",
        "\n",
        "#             if update_fields:\n",
        "#                 bulk_updates.append(UpdateOne({\"_id\": curr[\"_id\"]}, {\"$set\": update_fields}))\n",
        "\n",
        "#             # --- Flush bulk if needed ---\n",
        "#             if len(bulk_updates) >= BULK_LIMIT:\n",
        "#                 flush_updates()\n",
        "\n",
        "#             window.pop(0)\n",
        "#             count += 1\n",
        "\n",
        "#         # reset batch\n",
        "#         batch = []\n",
        "\n",
        "# # flush any remaining\n",
        "# flush_updates()\n",
        "# print(f\"Finished processing {count} rows.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SCRIPT TO PROCESS THE PREVIOULY COLLECTED LARGE DATA (OVER 1M+ RECORDS)\n",
        "\n",
        "from pymongo import UpdateOne\n",
        "\n",
        "# --- Config ---\n",
        "CHUNK_FETCH = 5000   # fetch this many docs at once\n",
        "PROCESS_SIZE = 4950  # process only 4900 docs (leave overlap)\n",
        "BULK_LIMIT = 1000    # flush bulk writes every 1000 updates\n",
        "\n",
        "bulk_updates = []\n",
        "carryover = []   # keep last 2 docs from previous batch\n",
        "count = 0\n",
        "\n",
        "def flush_updates():\n",
        "    \"\"\"Write accumulated bulk updates to DB.\"\"\"\n",
        "    global bulk_updates\n",
        "    if bulk_updates:\n",
        "        collection.bulk_write(bulk_updates, ordered=False)\n",
        "        bulk_updates = []\n",
        "\n",
        "# --- Stream through DB in batches ---\n",
        "batch_cursor = collection.find({}, {\"Server Timestamp\": 1, \"Value\": 1})\n",
        "batch = []\n",
        "\n",
        "for doc in batch_cursor:\n",
        "    batch.append(doc)\n",
        "    if len(batch) == CHUNK_FETCH:\n",
        "        # merge with carryover from previous batch\n",
        "        chunk = carryover + batch\n",
        "\n",
        "        # sort in memory by timestamp\n",
        "        chunk = sorted(chunk, key=lambda x: x[\"Server Timestamp\"])\n",
        "\n",
        "        # keep last 2 docs for overlap to next chunk\n",
        "        carryover = chunk[-2:]\n",
        "        process_chunk = chunk[:-2]   # leave last 2 aside\n",
        "\n",
        "        # process this chunk\n",
        "        window = []\n",
        "        for curr_doc in process_chunk:\n",
        "            window.append(curr_doc)\n",
        "            if len(window) < 3:\n",
        "                continue\n",
        "\n",
        "            prev, curr, nxt = window[0], window[1], window[2]\n",
        "            anomaly_flag = None\n",
        "            new_value = None\n",
        "\n",
        "            # ----------- RESET -----------\n",
        "            if curr[\"Value\"] < prev[\"Value\"] and nxt[\"Value\"] > prev[\"Value\"]:\n",
        "                anomaly_flag = \"Yes (Reset)\"\n",
        "\n",
        "            # ----------- FAULTY SENSOR -----------\n",
        "            elif curr[\"Value\"] == 0:\n",
        "                zero_block = [curr]\n",
        "                while len(window) >= 3 and window[1][\"Value\"] == 0:\n",
        "                    zero_block.append(window[1])\n",
        "                    window.pop(1)\n",
        "\n",
        "                if len(window) >= 3:\n",
        "                    after_val = window[1][\"Value\"]\n",
        "                    if after_val < prev[\"Value\"]:\n",
        "                        new_value = (prev[\"Value\"] + after_val) / 2\n",
        "                        for z in zero_block:\n",
        "                            bulk_updates.append(UpdateOne(\n",
        "                                {\"_id\": z[\"_id\"]},\n",
        "                                {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                            ))\n",
        "                    else:\n",
        "                        bulk_updates.append(UpdateOne(\n",
        "                            {\"_id\": zero_block[0][\"_id\"]},\n",
        "                            {\"$set\": {\"Anomaly Flag\": \"Yes (Reset)\"}}\n",
        "                        ))\n",
        "                else:  # dataset finished → copy prev\n",
        "                    new_value = prev[\"Value\"]\n",
        "                    for z in zero_block:\n",
        "                        bulk_updates.append(UpdateOne(\n",
        "                            {\"_id\": z[\"_id\"]},\n",
        "                            {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                        ))\n",
        "                continue\n",
        "\n",
        "            # ----------- NEGATIVE DIFFERENCE -----------\n",
        "            elif nxt[\"Value\"] > curr[\"Value\"]:\n",
        "                anomaly_flag = \"Yes (Negative Difference)\"\n",
        "\n",
        "            # --- Add to bulk ---\n",
        "            update_fields = {}\n",
        "            if anomaly_flag:\n",
        "                update_fields[\"Anomaly Flag\"] = anomaly_flag\n",
        "            if new_value is not None:\n",
        "                update_fields[\"Value\"] = new_value\n",
        "\n",
        "            if update_fields:\n",
        "                bulk_updates.append(UpdateOne({\"_id\": curr[\"_id\"]}, {\"$set\": update_fields}))\n",
        "\n",
        "            # --- Flush bulk if needed ---\n",
        "            if len(bulk_updates) >= BULK_LIMIT:\n",
        "                flush_updates()\n",
        "\n",
        "            window.pop(0)\n",
        "            count += 1\n",
        "\n",
        "            # --- NEW: Log every 50k processed rows ---\n",
        "            if count % 50000 == 0:\n",
        "                print(f\"Processed {count} rows...\")\n",
        "\n",
        "        # reset batch\n",
        "        batch = []\n",
        "\n",
        "# flush any remaining\n",
        "flush_updates()\n",
        "print(f\"Finished processing {count} rows.\")\n"
      ],
      "metadata": {
        "id": "bkNeJm3_7sQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCRIPT FOR SCHEDULER TO PROCESS LAST HOUR RECORDS WITH THE LOGGING INFO\n",
        "\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from pymongo import MongoClient, UpdateOne\n",
        "\n",
        "# --- Config ---\n",
        "BUFFER_SIZE = 10       # keep last N records as buffer (editable)\n",
        "PROCESS_SIZE = 60      # process this many records per run (editable)\n",
        "BULK_LIMIT = 1000      # flush bulk writes every 1000 updates\n",
        "\n",
        "def flush_updates(bulk_updates):\n",
        "    \"\"\"Write accumulated bulk updates to DB.\"\"\"\n",
        "    if bulk_updates:\n",
        "        collection.bulk_write(bulk_updates, ordered=False)\n",
        "        return len(bulk_updates)\n",
        "    return 0\n",
        "\n",
        "def process_last_hour():\n",
        "    start_time = time.time()\n",
        "    now = datetime.utcnow()\n",
        "    one_hour_ago = now - timedelta(hours=1)\n",
        "\n",
        "    # Fetch last 1 hour records (sorted by timestamp)\n",
        "    records = list(collection.find(\n",
        "        {\"Server Timestamp\": {\"$gte\": one_hour_ago, \"$lt\": now}},\n",
        "        {\"Server Timestamp\": 1, \"Value\": 1}\n",
        "    ).sort(\"Server Timestamp\", 1))\n",
        "\n",
        "    total_records = len(records)\n",
        "\n",
        "    if total_records <= BUFFER_SIZE:\n",
        "        print(f\"[{now}] Not enough records ({total_records}) to process. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Keep buffer, process rest\n",
        "    process_records = records[:-BUFFER_SIZE]\n",
        "    process_records = process_records[:PROCESS_SIZE]   # only process up to PROCESS_SIZE\n",
        "    bulk_updates = []\n",
        "    count = 0\n",
        "    modifications = 0\n",
        "    window = []\n",
        "\n",
        "    for curr_doc in process_records:\n",
        "        window.append(curr_doc)\n",
        "        if len(window) < 3:\n",
        "            continue\n",
        "\n",
        "        prev, curr, nxt = window[0], window[1], window[2]\n",
        "        anomaly_flag = None\n",
        "        new_value = None\n",
        "\n",
        "        # ----------- RESET -----------\n",
        "        if curr[\"Value\"] < prev[\"Value\"] and nxt[\"Value\"] > prev[\"Value\"]:\n",
        "            anomaly_flag = \"Yes (Reset)\"\n",
        "\n",
        "        # ----------- FAULTY SENSOR -----------\n",
        "        elif curr[\"Value\"] == 0:\n",
        "            zero_block = [curr]\n",
        "            while len(window) >= 3 and window[1][\"Value\"] == 0:\n",
        "                zero_block.append(window[1])\n",
        "                window.pop(1)\n",
        "\n",
        "            if len(window) >= 3:\n",
        "                after_val = window[1][\"Value\"]\n",
        "                if after_val < prev[\"Value\"]:\n",
        "                    new_value = (prev[\"Value\"] + after_val) / 2\n",
        "                    for z in zero_block:\n",
        "                        bulk_updates.append(UpdateOne(\n",
        "                            {\"_id\": z[\"_id\"]},\n",
        "                            {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                        ))\n",
        "                        modifications += 1\n",
        "                else:\n",
        "                    bulk_updates.append(UpdateOne(\n",
        "                        {\"_id\": zero_block[0][\"_id\"]},\n",
        "                        {\"$set\": {\"Anomaly Flag\": \"Yes (Reset)\"}}\n",
        "                    ))\n",
        "                    modifications += 1\n",
        "            else:  # dataset finished → copy prev\n",
        "                new_value = prev[\"Value\"]\n",
        "                for z in zero_block:\n",
        "                    bulk_updates.append(UpdateOne(\n",
        "                        {\"_id\": z[\"_id\"]},\n",
        "                        {\"$set\": {\"Anomaly Flag\": \"Yes (Faulty)\", \"Value\": new_value}}\n",
        "                    ))\n",
        "                    modifications += 1\n",
        "            continue\n",
        "\n",
        "        # ----------- NEGATIVE DIFFERENCE -----------\n",
        "        elif nxt[\"Value\"] > curr[\"Value\"]:\n",
        "            anomaly_flag = \"Yes (Negative Difference)\"\n",
        "\n",
        "        # --- Add to bulk ---\n",
        "        update_fields = {}\n",
        "        if anomaly_flag:\n",
        "            update_fields[\"Anomaly Flag\"] = anomaly_flag\n",
        "        if new_value is not None:\n",
        "            update_fields[\"Value\"] = new_value\n",
        "\n",
        "        if update_fields:\n",
        "            bulk_updates.append(UpdateOne({\"_id\": curr[\"_id\"]}, {\"$set\": update_fields}))\n",
        "            modifications += 1\n",
        "\n",
        "        # --- Flush bulk if needed ---\n",
        "        if len(bulk_updates) >= BULK_LIMIT:\n",
        "            modifications += flush_updates(bulk_updates)\n",
        "            bulk_updates = []\n",
        "\n",
        "        window.pop(0)\n",
        "        count += 1\n",
        "\n",
        "    # Flush remaining\n",
        "    modifications += flush_updates(bulk_updates)\n",
        "\n",
        "    # --- Logging ---\n",
        "    end_time = time.time()\n",
        "    duration = round(end_time - start_time, 2)\n",
        "    print(f\"\"\"\n",
        "========== HOURLY PROCESS REPORT ==========\n",
        "Time Window   : {one_hour_ago} → {now}\n",
        "Records Found : {total_records}\n",
        "Processed     : {count}\n",
        "Modifications : {modifications}\n",
        "Time Taken    : {duration} seconds\n",
        "===========================================\n",
        "\"\"\")\n",
        "\n",
        "# --- Run Once (Scheduler will call this every hour) ---\n",
        "if __name__ == \"__main__\":\n",
        "    process_last_hour()\n"
      ],
      "metadata": {
        "id": "ygXYzCKJJNzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
